{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook: TCN Model Training and Evaluation\n",
    "\n",
    "In this notebook, we will load the stock data, preprocess it, build and train a TCN-based model (using our hybrid functions), and then evaluate and plot the results. All functions are imported from the `DL_TCN` module.\n",
    "\n",
    "## 1. Importing Libraries and Module\n",
    "\n",
    "First, we import the required libraries and the module containing our functions.\n"
   ],
   "id": "21d666887653535f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import layers, models, Input, Model, regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tcn import TCN"
   ],
   "id": "de50817e1d22854d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def R2(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(\n",
    "        tf.square(y_true - y_pred)\n",
    "    )\n",
    "    ss_tot = tf.reduce_sum(\n",
    "        tf.square(y_true - tf.reduce_mean(y_true))\n",
    "    )\n",
    "    \n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "def ACCURACY_5(y_true, y_pred):\n",
    "    # Erreur relative : |y_true - y_pred| / (|y_true| + epsilon) on evite les divisions par 0\n",
    "    error = tf.abs(\n",
    "        (y_true - y_pred) / (tf.abs(y_true))\n",
    "    )\n",
    "    correct = tf.cast(\n",
    "        error <= 0.05,\n",
    "        tf.float32\n",
    "    )\n",
    "\n",
    "    return tf.reduce_mean(correct)\n",
    "\n",
    "def CREATE_SEQUENCES(values, sequence_length=60):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(values)):\n",
    "        X_window = values[i - sequence_length: i]\n",
    "        y_value = values[i] \n",
    "        X.append(X_window)\n",
    "        y.append(y_value)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def NN_MODEL(input_shape, learning_rate=0.0005):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        TCN(\n",
    "            nb_filters=1,\n",
    "            kernel_size=200,\n",
    "            nb_stacks=1,\n",
    "            dilations=[1, 2, 4, 8],\n",
    "            padding='causal',\n",
    "            dropout_rate=0.2,\n",
    "            return_sequences=True\n",
    "        ),\n",
    "\n",
    "        layers.LSTM(10),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[\n",
    "            'mean_absolute_error',\n",
    "            R2,\n",
    "            ACCURACY_5\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def TCN_2(input_shape, num_filters=32, kernel_size=2, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        padding='causal',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Conv1D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        padding='causal',\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Connexion rÃ©siduelle\n",
    "    if inputs.shape[-1] != x.shape[-1]:\n",
    "        inputs_res = layers.Conv1D(num_filters, kernel_size=1, padding='same')(inputs)\n",
    "    else:\n",
    "        inputs_res = inputs\n",
    "    \n",
    "    x = layers.Add()([inputs_res, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    model = Model(inputs, x, name=\"TCN_2\")\n",
    "    return model\n",
    "\n",
    "def NN_MODEL_2(input_shape, tcn_filters=32, lstm_units=50, dropout_rate=0.2, learning_rate=0.0005):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    tcn_out = TCN_2(input_shape=input_shape,\n",
    "                              num_filters=tcn_filters,\n",
    "                              kernel_size=2,\n",
    "                              dropout_rate=dropout_rate)(inputs)\n",
    "    \n",
    "    lstm_out = layers.LSTM(lstm_units)(tcn_out)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=\"Hybrid_TCN_LSTM_Model\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ],
   "id": "5bebe178ba90dcca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "We load our stock market data from a CSV file and extract the \"close\" price column.\n"
   ],
   "id": "7dfcee9438defb82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV data\n",
    "df = pd.read_csv(\"/Datasets/NASDAQ_100.csv\")\n",
    "\n",
    "# Extract the 'close' column and reshape to (num_samples, 1)\n",
    "close_prices = df['close'].values.reshape(-1, 1)\n",
    "\n",
    "print(\"Data shape:\", close_prices.shape)\n"
   ],
   "id": "f7ce07644e7f5fe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Next, we normalize the close prices using a MinMaxScaler.\n"
   ],
   "id": "cbb313bed44a1a73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize and fit the scaler\n",
    "scaler = StandardScaler()\n",
    "scaled_prices = scaler.fit_transform(close_prices)\n",
    "\n",
    "# Check a few values\n",
    "print(\"Scaled prices (first 5 rows):\")\n",
    "scaled_prices[:5]\n"
   ],
   "id": "22aa2f53cb080759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Creating Sequences\n",
    "\n",
    "We use a sliding window approach to create input sequences and corresponding targets.\n",
    "Here, we use a sequence length of 1000 days to predict the following day.\n"
   ],
   "id": "5e8505531d040e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set sequence length (number of input days)\n",
    "sequence_length = 2000\n",
    "\n",
    "# Create sequences using the function from DL_TCN\n",
    "X, y = CREATE_SEQUENCES(scaled_prices, sequence_length=sequence_length)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ],
   "id": "8ff1cccbbd1c6943",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Splitting the Dataset\n",
    "\n",
    "We split the data into training (70%), validation (15%), and test (15%) sets.\n"
   ],
   "id": "e6abf6f97cc5455e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine split sizes\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "# Create train, validation, and test splits\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation shapes:\", X_val.shape, y_val.shape)\n",
    "print(\"Test shapes:\", X_test.shape, y_test.shape)\n"
   ],
   "id": "af39ece9cee9b1c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Building the Model\n",
    "\n",
    "We build our hybrid TCN+LSTM model using the function from the module.  \n",
    "Our model takes sequences of shape (sequence_length, 1) and predicts a single value.\n"
   ],
   "id": "9ac2e20705d730ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the model using the function from DL_TCN\n",
    "model = NN_MODEL(input_shape=(sequence_length, 1), learning_rate=0.0005)\n",
    "model.summary()\n"
   ],
   "id": "768904dd56d6edc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Training the Model\n",
    "\n",
    "We train the model using EarlyStopping and ReduceLROnPlateau to avoid overfitting.\n"
   ],
   "id": "b224b4b148ea6366"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        #early_stopping,\n",
    "        reduce_lr\n",
    "    ]\n",
    ")\n"
   ],
   "id": "f088f438fe5fc028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Plotting the Training Loss\n",
    "\n",
    "We now plot the training and validation loss curves (MSE) over epochs.\n"
   ],
   "id": "1e1757a8c5e33041"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Loss over Epochs (TCN Model)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "3dc3c92d495bbb8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Model Evaluation and Predictions\n",
    "\n",
    "Next, we evaluate the model on the test set and predict closing prices.\n"
   ],
   "id": "4357e3cc36e56864"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate on test set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test)\n"
   ],
   "id": "6d2ec1b660d3c9d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Inverse Scaling and Metrics Calculation\n",
    "\n",
    "We inverse the scaling to get the original closing price values, then calculate performance metrics (RÂ², MAE, etc.).\n"
   ],
   "id": "31c9973e357ed4df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inverse transform the predictions and true values\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate metrics using functions from scikit-learn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "rmse = np.sqrt(np.mean((y_test_unscaled - y_pred_unscaled) ** 2))\n",
    "mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
    "r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
    "mape = mean_absolute_percentage_error(y_test_unscaled, y_pred_unscaled) * 100\n",
    "accuracy = ACCURACY_5(y_test_unscaled, y_pred_unscaled)\n",
    "\n",
    "print(\"----- Test Set Performance -----\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RÂ²: {r2:.4f}\")\n",
    "print(f\"Accuracy (5% tolerance): {accuracy:.2f}%\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n"
   ],
   "id": "9f4c89e76d5d295d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Plotting Predictions vs. Real Values\n",
    "\n",
    "Finally, we plot the predictions against the real closing prices for the first 100 test samples.\n"
   ],
   "id": "e8311f0a1cca29a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_plot = 100\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_unscaled[:n_plot], label='Real Price')\n",
    "plt.plot(y_pred_unscaled[:n_plot], label='Predicted Price')\n",
    "plt.xlabel(\"Test Sample Index\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.title(\"TCN Model Predictions vs. Real Values\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "202cd8c8615f8cae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
